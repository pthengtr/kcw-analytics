{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pthengtr/kcw-analytics/blob/main/notebooks/01_supabase_upload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-KiiutlOQxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48556e91-9851-4282-ec2c-dfb69deace2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.11\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o_U8J6l6fEwt",
        "outputId": "89201898-2dd1-456d-c6fa-671d20f7b132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "folder = \"/content/drive/MyDrive/kcw_analytics/01_raw\"\n",
        "\n",
        "data = {}\n",
        "\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".csv\"):\n",
        "        path = os.path.join(folder, file)\n",
        "        data[file] = pd.read_csv(\n",
        "            path,\n",
        "            dtype={\n",
        "              \"BCODE\": \"string\",\n",
        "              \"ITEMNO\": \"string\",\n",
        "              \"BILLNO\": \"string\",\n",
        "            },\n",
        "            encoding=\"utf-8-sig\",\n",
        "            low_memory=False   # stops chunk guessing\n",
        "        )\n",
        "        print(f\"Loaded: {file} -> {data[file].shape}\")"
      ],
      "metadata": {
        "id": "IHvePYV6fKH0",
        "outputId": "deaf165d-0457-4f0e-d56c-7c220b420b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: raw_inventory_hq_2024.csv -> (4983, 8)\n",
            "Loaded: raw_syp_pimas_purchase_bills.csv -> (2881, 49)\n",
            "Loaded: raw_syp_pidet_purchase_lines.csv -> (26919, 41)\n",
            "Loaded: raw_syp_sidet_sales_lines.csv -> (35191, 38)\n",
            "Loaded: raw_syp_simas_sales_bills.csv -> (11923, 49)\n",
            "Loaded: raw_hq_pimas_purchase_bills.csv -> (83416, 49)\n",
            "Loaded: raw_hq_pidet_purchase_lines.csv -> (248835, 41)\n",
            "Loaded: raw_hq_icmas_products.csv -> (114874, 94)\n",
            "Loaded: raw_hq_sidet_sales_lines.csv -> (1200038, 38)\n",
            "Loaded: raw_hq_simas_sales_bills.csv -> (486118, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def filter_last_year_from_latest(\n",
        "    df: pd.DataFrame,\n",
        "    date_col: str = \"BILLDATE\",\n",
        "    *,\n",
        "    years: int = 1,\n",
        "    keep_invalid: bool = False,\n",
        "    inplace: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Keep rows where `date_col` is within `years` years back from the latest date in that column.\n",
        "\n",
        "    - Parses dates with pd.to_datetime(errors=\"coerce\")\n",
        "    - If keep_invalid=False (default): drops rows where date_col can't be parsed.\n",
        "    - If keep_invalid=True: keeps invalid-date rows (NaT) in the output.\n",
        "\n",
        "    Returns a filtered copy unless inplace=True (then returns the same df reference).\n",
        "    \"\"\"\n",
        "    if date_col not in df.columns:\n",
        "        raise KeyError(f\"Column not found: {date_col}\")\n",
        "\n",
        "    out = df if inplace else df.copy()\n",
        "\n",
        "    # Parse to datetime safely\n",
        "    dt = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
        "\n",
        "    latest = dt.max()\n",
        "    if pd.isna(latest):\n",
        "        # No valid dates at all\n",
        "        return out if keep_invalid else out.iloc[0:0].copy()\n",
        "\n",
        "    cutoff = latest - pd.DateOffset(years=years)\n",
        "\n",
        "    mask_recent = dt >= cutoff\n",
        "    if keep_invalid:\n",
        "        mask = mask_recent | dt.isna()\n",
        "    else:\n",
        "        mask = mask_recent\n",
        "\n",
        "    return out.loc[mask].copy()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# df_1y = filter_last_year_from_latest(df, \"BILLDATE\")\n",
        "# df_1y = filter_last_year_from_latest(df_pidet, \"JOURDATE\")"
      ],
      "metadata": {
        "id": "_tVN_p_BDuBX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BRANCH = 'hq'"
      ],
      "metadata": {
        "id": "-y8ty75bMvul"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sidet = data[f'raw_{BRANCH}_sidet_sales_lines.csv']\n",
        "df_simas = data[f'raw_{BRANCH}_simas_sales_bills.csv']\n",
        "df_pidet = data[f'raw_{BRANCH}_pidet_purchase_lines.csv']\n",
        "df_pimas = data[f'raw_{BRANCH}_pimas_purchase_bills.csv']\n",
        "df_icmas = data[f'raw_hq_icmas_products.csv']\n",
        "\n",
        "df_sidet = filter_last_year_from_latest(df_sidet, \"BILLDATE\")\n",
        "df_simas = filter_last_year_from_latest(df_simas, \"BILLDATE\")\n",
        "df_pidet = filter_last_year_from_latest(df_pidet, \"BILLDATE\")\n",
        "df_pimas = filter_last_year_from_latest(df_pimas, \"BILLDATE\")\n",
        "\n",
        "print(len(df_sidet))\n",
        "print(len(df_simas))\n",
        "print(len(df_pidet))\n",
        "print(len(df_pimas))\n",
        "print(len(df_icmas))\n"
      ],
      "metadata": {
        "id": "CfLHtSCxDPIS",
        "outputId": "a55babbe-016b-4d9b-cda2-78166d06b0a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "193201\n",
            "61913\n",
            "39081\n",
            "10846\n",
            "114874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import psycopg2\n",
        "\n",
        "conn = psycopg2.connect(\n",
        "    host=\"aws-0-ap-southeast-1.pooler.supabase.com\",   # paste yours here\n",
        "    port=5432,\n",
        "    dbname=\"postgres\",\n",
        "    user=\"postgres.jdzitzsucntqbjvwiwxm\",        # note the dot + project ref\n",
        "    password=\"3h3aAixsyK4X762r\",\n",
        "    sslmode=\"require\",\n",
        ")\n",
        "print(\"Connected via pooler ✅\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk89s4SHTD5i",
        "outputId": "b0034f71-1c98-40fc-91c4-3d54bbd54d82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected via pooler ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import csv\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "\n",
        "\n",
        "def _qident(name: str) -> str:\n",
        "    \"\"\"Quote an identifier safely for Postgres.\"\"\"\n",
        "    return '\"' + name.replace('\"', '\"\"') + '\"'\n",
        "\n",
        "\n",
        "def _get_table_columns(conn, schema: str, table: str) -> list[str]:\n",
        "    sql = \"\"\"\n",
        "    SELECT column_name\n",
        "    FROM information_schema.columns\n",
        "    WHERE table_schema = %s AND table_name = %s\n",
        "    ORDER BY ordinal_position\n",
        "    \"\"\"\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(sql, (schema, table))\n",
        "        rows = cur.fetchall()\n",
        "    return [r[0] for r in rows]\n",
        "\n",
        "\n",
        "def bulk_upsert_via_stage(\n",
        "    conn,\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    target_table: str,\n",
        "    schema: str = \"public\",\n",
        "    stage_suffix: str = \"_stage\",\n",
        "    pk_cols: list[str] = None,\n",
        "    truncate_stage: bool = True,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Bulk upsert df into target_table by:\n",
        "      TRUNCATE stage\n",
        "      COPY df -> stage\n",
        "      INSERT target SELECT stage ON CONFLICT(pk) DO UPDATE\n",
        "    Returns basic stats: rows_in_df, target_table, stage_table.\n",
        "    \"\"\"\n",
        "    if pk_cols is None:\n",
        "        pk_cols = [\"ID\"]\n",
        "\n",
        "    if df is None or len(df) == 0:\n",
        "        return {\"rows_in_df\": 0, \"target_table\": target_table, \"stage_table\": f\"{target_table}{stage_suffix}\"}\n",
        "\n",
        "    stage_table = f\"{target_table}{stage_suffix}\"\n",
        "\n",
        "    # Fetch canonical column order from the DB (target table)\n",
        "    target_cols = _get_table_columns(conn, schema, target_table)\n",
        "    if not target_cols:\n",
        "        raise ValueError(f\"Target table not found: {schema}.{target_table}\")\n",
        "\n",
        "    stage_cols = _get_table_columns(conn, schema, stage_table)\n",
        "    if not stage_cols:\n",
        "        raise ValueError(f\"Stage table not found: {schema}.{stage_table}\")\n",
        "\n",
        "    if target_cols != stage_cols:\n",
        "        raise ValueError(\n",
        "            f\"Target/stage column mismatch.\\n\"\n",
        "            f\"Target({len(target_cols)}): {target_cols}\\n\"\n",
        "            f\"Stage({len(stage_cols)}):  {stage_cols}\"\n",
        "        )\n",
        "\n",
        "    missing = [c for c in target_cols if c not in df.columns]\n",
        "    extra = [c for c in df.columns if c not in target_cols]\n",
        "    if missing:\n",
        "        raise ValueError(f\"DF is missing columns required by table: {missing}\")\n",
        "    if extra:\n",
        "        # Not fatal, but usually indicates you loaded the wrong dataframe\n",
        "        print(f\"Warning: DF has extra columns not in table (they will be ignored): {extra}\")\n",
        "\n",
        "    # Reorder and drop extras; keep exactly table columns\n",
        "    df2 = df[target_cols].copy()\n",
        "\n",
        "    # Convert NaN -> None so COPY writes empty fields\n",
        "    df2 = df2.where(pd.notnull(df2), None)\n",
        "\n",
        "    fq_target = f\"{_qident(schema)}.{_qident(target_table)}\"\n",
        "    fq_stage = f\"{_qident(schema)}.{_qident(stage_table)}\"\n",
        "\n",
        "    cols_sql = \", \".join(_qident(c) for c in target_cols)\n",
        "    pk_sql = \", \".join(_qident(c) for c in pk_cols)\n",
        "\n",
        "    # Build SET clause excluding PK columns\n",
        "    non_pk_cols = [c for c in target_cols if c not in set(pk_cols)]\n",
        "    if not non_pk_cols:\n",
        "        raise ValueError(\"No non-PK columns to update. Check pk_cols.\")\n",
        "\n",
        "    set_sql = \", \".join(f\"{_qident(c)} = EXCLUDED.{_qident(c)}\" for c in non_pk_cols)\n",
        "\n",
        "    merge_sql = f\"\"\"\n",
        "    INSERT INTO {fq_target} ({cols_sql})\n",
        "    SELECT {cols_sql}\n",
        "    FROM {fq_stage}\n",
        "    ON CONFLICT ({pk_sql}) DO UPDATE\n",
        "    SET {set_sql}\n",
        "    \"\"\"\n",
        "\n",
        "    # Write DF to CSV buffer (no header), then COPY\n",
        "    buf = io.StringIO()\n",
        "    writer = csv.writer(buf, quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\")\n",
        "    for row in df2.itertuples(index=False, name=None):\n",
        "        writer.writerow([\"\" if v is None else v for v in row])\n",
        "    buf.seek(0)\n",
        "\n",
        "    try:\n",
        "        with conn.cursor() as cur:\n",
        "            if truncate_stage:\n",
        "                cur.execute(f\"TRUNCATE TABLE {fq_stage};\")\n",
        "\n",
        "            cur.copy_expert(\n",
        "                f\"COPY {fq_stage} ({cols_sql}) FROM STDIN WITH (FORMAT CSV)\",\n",
        "                buf\n",
        "            )\n",
        "\n",
        "            cur.execute(merge_sql)\n",
        "\n",
        "        conn.commit()\n",
        "        return {\n",
        "            \"rows_in_df\": len(df2),\n",
        "            \"target_table\": f\"{schema}.{target_table}\",\n",
        "            \"stage_table\": f\"{schema}.{stage_table}\",\n",
        "        }\n",
        "\n",
        "    except Exception:\n",
        "        conn.rollback()\n",
        "        raise"
      ],
      "metadata": {
        "id": "hRXzDbkKBv1c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: upsert product master\n",
        "result = bulk_upsert_via_stage(\n",
        "    conn,\n",
        "    df_sidet,\n",
        "    target_table=f\"raw_{BRANCH}_sidet_lines\",\n",
        "    pk_cols=[\"ID\"],\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "dFv6twroNXw5",
        "outputId": "773b665b-5602-49ca-8c8f-7caaf23c8ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rows_in_df': 193201, 'target_table': 'public.raw_hq_sidet_lines', 'stage_table': 'public.raw_hq_sidet_lines_stage'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: upsert product master\n",
        "result = bulk_upsert_via_stage(\n",
        "    conn,\n",
        "    df_pidet,\n",
        "    target_table=f\"raw_{BRANCH}_pidet_lines\",\n",
        "    pk_cols=[\"ID\"],\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "bybgWReHNYui",
        "outputId": "55fa07fb-c78e-4542-cc27-716a9cf54c61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rows_in_df': 39081, 'target_table': 'public.raw_hq_pidet_lines', 'stage_table': 'public.raw_hq_pidet_lines_stage'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: upsert product master\n",
        "result = bulk_upsert_via_stage(\n",
        "    conn,\n",
        "    df_simas,\n",
        "    target_table=f\"raw_{BRANCH}_simas_bills\",\n",
        "    pk_cols=[\"ID\"],\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "QOWQEEbdNZOE",
        "outputId": "95dec053-4bfc-4909-f0a7-29a63a197304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rows_in_df': 61913, 'target_table': 'public.raw_hq_simas_bills', 'stage_table': 'public.raw_hq_simas_bills_stage'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: upsert product master\n",
        "result = bulk_upsert_via_stage(\n",
        "    conn,\n",
        "    df_pimas,\n",
        "    target_table=f\"raw_{BRANCH}_pimas_bills\",\n",
        "    pk_cols=[\"ID\"],\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "I6eE-MhSNaQK",
        "outputId": "462ad3a1-9e39-4edb-e4da-54dee027246a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rows_in_df': 10846, 'target_table': 'public.raw_hq_pimas_bills', 'stage_table': 'public.raw_hq_pimas_bills_stage'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: upsert product master\n",
        "result = bulk_upsert_via_stage(\n",
        "    conn,\n",
        "    df_icmas,\n",
        "    target_table=f\"raw_{BRANCH}_icmas_products\",\n",
        "    pk_cols=[\"ID\"],\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "wxtFbxMxNyFZ",
        "outputId": "93c6387f-932e-451e-cc68-2756a611227f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rows_in_df': 114874, 'target_table': 'public.raw_hq_icmas_products', 'stage_table': 'public.raw_hq_icmas_products_stage'}\n"
          ]
        }
      ]
    }
  ]
}